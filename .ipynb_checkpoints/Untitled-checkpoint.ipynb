{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42129f4a-4e69-4be9-bea6-b32f1ff720fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "!pip install sentence-transformers\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "\n",
    "\n",
    "# Download NLTK resources (if not already done)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Greek SpaCy model (Ensure this model is installed)\n",
    "# Make sure you have downloaded the Greek model: `!python -m spacy download el_core_news_sm`\n",
    "nlp = spacy.load(\"el_core_news_sm\")\n",
    "\n",
    "# Define custom stopwords (English, Greek, and any additional custom ones)\n",
    "english_stopwords = set(stopwords.words('english'))  # NLTK English stopwords\n",
    "greek_stopwords = set(stopwords.words('greek'))  # NLTK Greek stopwords\n",
    "\n",
    "# Add your own custom stopwords\n",
    "custom_stopwords = {\"θέλω\", \"θελω\", \"ο\", \"to\", \"είμαι\", 'ειμαι', \"επιθυμώ\", \"να\", \"για\", \"μου\", \"και\", \"πώς\", \"κάνω\", \"έχω\", \"μπορώ\", 'ένας'}\n",
    "\n",
    "# **Step 1: Tokenization + Lemmatization + Stopword Removal Function**\n",
    "def preprocess_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    This function performs the following:\n",
    "    1. Tokenization: Splits the text into words (tokens).\n",
    "    2. Lemmatization: Converts each word into its base form.\n",
    "    3. Stopword Removal: Removes predefined stopwords (English, Greek, and custom stopwords).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Tokenize the text using SpaCy (splitting the sentence into individual tokens/words)\n",
    "    doc = nlp(text)\n",
    "    print(f\"Original Tokens: {[token.text for token in doc]}\")  # Print the original tokens\n",
    "\n",
    "    # Step 3: Lemmatization - Convert each word to its base form (lemma)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    print(f\"Lemmatized Tokens: {lemmatized_tokens}\")  # Print the lemmatized tokens\n",
    "\n",
    "    # Step 4: Exclude stopwords (English, Greek, and custom)\n",
    "    # Printing stopwords used\n",
    "    print(f\"English Stopwords: {english_stopwords}\")\n",
    "    print(f\"Greek Stopwords: {greek_stopwords}\")\n",
    "    print(f\"Custom Stopwords: {custom_stopwords}\")\n",
    "\n",
    "    # Remove stopwords based on the lemmatized tokens\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in english_stopwords\n",
    "                       and token not in greek_stopwords and token not in custom_stopwords]\n",
    "\n",
    "    print(f\"Filtered Tokens (No Stopwords): {filtered_tokens}\")  # Print tokens after stopwords are removed\n",
    "\n",
    "    # Join the remaining words back into a sentence\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# **Step 2: Example dataset**\n",
    "# You can replace this with your own dataset (this is just a sample)\n",
    "file_path = 'C:/Users/Katerina/Downloads/Customer Utterances.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create DataFrame from the sample dataset\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# **Step 3: Apply Tokenization, Lemmatization, and Stopword Removal to the Entire Dataset**\n",
    "# Apply the `preprocess_and_lemmatize` function to each row in the 'Utterance' column\n",
    "df['Processed'] = df['Utterance'].apply(preprocess_and_lemmatize)\n",
    "\n",
    "# **Step 4: Display the Processed DataFrame (After Preprocessing)**\n",
    "print(\"Processed Data (After Lemmatization and Stopword Removal):\")\n",
    "print(df[['Utterance', 'Processed']])\n",
    "\n",
    "# **Step 5: Sentence Embeddings using Sentence-BERT**\n",
    "# Initialize the Sentence-BERT model (multilingual model that works for both English and Greek)\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Generate embeddings for each processed sentence\n",
    "embeddings = model.encode(df['Processed'].tolist())\n",
    "\n",
    "# **Step 6: Clustering using KMeans (Example: Finding Optimal Clusters)**\n",
    "# We will use KMeans clustering to group similar queries together.\n",
    "\n",
    "# **Step 6a: Elbow Method to Find Optimal Number of Clusters**\n",
    "inertia = []  # Store inertia values for the Elbow method\n",
    "for k in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve to find the optimal k\n",
    "plt.plot(range(1, 15), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "\n",
    "# **Step 6b: Silhouette Score for Cluster Evaluation**\n",
    "silhouette_scores = []\n",
    "for k in range(2, 15):  # Start from 2 clusters (as 1 cluster isn't useful for silhouette score)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "    score = silhouette_score(embeddings, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores to evaluate clustering quality\n",
    "plt.plot(range(2, 15), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Method')\n",
    "plt.show()\n",
    "\n",
    "# **Step 7: Perform KMeans Clustering (Choose the optimal number of clusters)**\n",
    "num_clusters = 4  # Based on your elbow/silhouette analysis\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# **Step 8: Mapping Clusters to Intents (Example)**\n",
    "\n",
    "# Define a simple manual mapping for cluster labels to intent labels\n",
    "cluster_labels_mapping = {\n",
    "    0: \"Payment Inquiry\",\n",
    "    1: \"Account Information\",\n",
    "    2: \"Data Usage Inquiry\",\n",
    "    3: \"Service Cancellation\"\n",
    "}\n",
    "\n",
    "# Print the cluster labels and their corresponding intents\n",
    "for cluster, label in cluster_labels_mapping.items():\n",
    "    print(f\"Cluster {cluster}: {label}\")\n",
    "\n",
    "# **Step 9: Test the Intent Prediction Function**\n",
    "def predict_intent(query):\n",
    "    # Preprocess and lemmatize the query\n",
    "    processed_query = preprocess_and_lemmatize(query)\n",
    "\n",
    "    # Generate the embedding for the query\n",
    "    query_embedding = model.encode([processed_query])\n",
    "\n",
    "    # Predict the cluster for the query\n",
    "    cluster = kmeans.predict(query_embedding)[0]\n",
    "\n",
    "    # Return the intent label based on the predicted cluster\n",
    "    return cluster_labels_mapping.get(cluster, 'Unknown Intent')\n",
    "\n",
    "# Test the intent prediction function\n",
    "while True:\n",
    "    query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    intent = predict_intent(query)\n",
    "    print(f\"Predicted Intent: {intent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ced0f-321f-4baa-848c-ccd99900515d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
